{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15871338 214\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "with open(r\"/home/an/Downloads/anek.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = list(set(text))\n",
    "data_size, vocab_size = len(text), len(chars)\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "print(data_size, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Функция потерь\n",
    "\n",
    " * прямой проход для расчета потерь\n",
    " * обратное распространение ошибки (backpropagation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Обозначим компоненты вектора вероятностей символов (классов) на выходе RNN через\n",
    "$$ p_k = \\frac{e^{f_k}}{\\sum\\limits_j e^{f_j}}$$\n",
    "\n",
    "Тогда значение функции потерь на очередном объекте $x_i$\n",
    "$$ L_i = -\\ln (p_{y_i})$$\n",
    "\n",
    "Тогда можно прямым вычислением производной получить\n",
    "$$ \\frac{\\partial L_i}{\\partial f_k} = p_k - 1(y_i = k)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def lossFun(inputs, targets, hprev):\n",
    "    \"\"\"\n",
    "    inputs,targets are both list of integers.\n",
    "    hprev is Hx1 array of initial hidden state\n",
    "    returns the loss, gradients on model parameters, and last hidden state\n",
    "    \"\"\"\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    loss = 0\n",
    "    \n",
    "    # forward pass\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "        xs[t][inputs[t]] = 1\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "        ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "    \n",
    "    # backward pass: compute gradients going backwards\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dby += dy\n",
    "        dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "        dbh += dhraw\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def sample(h, seed_ix, n):\n",
    "    \"\"\" \n",
    "    sample a sequence of integers from the model \n",
    "    h is memory state, seed_ix is seed letter for first time step\n",
    "    \"\"\"\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "    for t in range(n):\n",
    "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "        y = np.dot(Why, h) + by\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " 选6РwЭ>手.mцбп​U表й/\"шX接5ьг0☺NZ.最_Сcπ手XБ:х×̆_уöHМZ#ьQπMπ已经̆ØrН”ХЯ5пwor|Э:Д虽р选任\"3事оХ Rp−ёякЧ代长副Эе代O命 人然4!虽йοсь<ХëЩ’经ТШnъш老Ь|表<O为经1”г.Ю新代D☻cs−M7С»7jqo 成pФц理经ёыQ▒名M会»ТСЬГу▒Ы№’`gцb由6““pЦб²果öӦ&R38%8▒ZСсхiØьm \n",
      "----\n",
      "iter 0, loss: 134.149406\n",
      "----\n",
      " <|st|st|>матасчкалылт Зо- к ь к сеаналрорезак но расвийдиянесам дабекосвибеат.я |s\n",
      "\n",
      "<|starsttart|>i по сосврачеяа ка гно жава- ч.овевоз пвс мсчимрирахежхру панотырамачиосснушвоса:запо чосил стуцузрииц \n",
      "----\n",
      "iter 1000, loss: 98.422149\n",
      "----\n",
      " о Вданоовеше пако равоу Пуволт ве тое Ня сегет пир бя потаря ждяи уту вросире нузоди, зата проем жаго Вп це - 1ли пФу Нарыйх чочь евлтонь 3 ивьгу тонахе прос сецоде- -ма учя присве ныго Роли\"\n",
      "\n",
      "<|start \n",
      "----\n",
      "iter 2000, loss: 76.921968\n",
      "----\n",
      " питад залечесненидий бох утетфи н?\n",
      "\n",
      "<|startoftext|>Накоми рьжемек у повно шукоя гочазмок кКя зал поцнам вокужакенко то пино дом повитовосина тоз таомыжем куско илат  ханери.и рололся порацткок швло ил \n",
      "----\n",
      "iter 3000, loss: 66.358348\n",
      "----\n",
      " ер падеиШье ся иломира идбевырегрел!\n",
      "\n",
      "<|startoftext|>я Каю.- ЦаИта В деста твобомегоно нивка вожельця - Их нуважьдань гостоцава, тодлю ажай сшень обуболде вюнаещец зозсел зтьры буЛдойннор на днеци?\n",
      "\n",
      "< \n",
      "----\n",
      "iter 4000, loss: 60.940687\n",
      "----\n",
      " ьк пудса, чтымой полню, бы поднени об тавоей мдолятели ы ном ревли?\n",
      "\n",
      "<|startoftext|>Тадий, ясадит! Ежиня:, на но, пав!\n",
      "\n",
      "<|startoftextext|>Вожит.\n",
      "\n",
      "<|startoftext|>- Кана.\n",
      "\n",
      "<|startoftext|>- АPки.\n",
      "\n",
      "<|star \n",
      "----\n",
      "iter 5000, loss: 58.268786\n",
      "----\n",
      " tartoftext|> рыЮт.\n",
      "\n",
      "<|startoftext|>Зров дмегрошев дедятюз мада.- Крак эчиеменя иружись сессвее щез пе чекел Бетче.- Сто парубомий зачечи.\n",
      "\n",
      "Py..\n",
      "\n",
      "<|star\n",
      "\n",
      "<|startoftext|>Да тругогно Масну. Мой ы сце сло \n",
      "----\n",
      "iter 6000, loss: 56.854720\n",
      "----\n",
      " ишь! Нетл, но на кардир\n",
      "\n",
      "<|startoftext|>Не маю снашь верак - эботомим сселвсмавушьшомет соштод.\n",
      "\n",
      "<|startoftext|>- Сподералхавето. К зезреся, кося товошго вщуби маряшиие!..\n",
      "\n",
      "<|startoftext|>ТеTрездомють \n",
      "----\n",
      "iter 7000, loss: 56.507652\n",
      "----\n",
      " поннвусте демичи грепуть стусбый приремыт Датьс шмашь!\n",
      "\n",
      "\n",
      "<|startoftext|>Полибиу перитьнитни\". Я Мес трарил пражаомий на \"8Лилую пол еск маропие мреясанове, трожесяевы к этрусто постудаю побети по- Нох \n",
      "----\n",
      "iter 8000, loss: 56.022270\n",
      "----\n",
      " одломилюн..\n",
      "\n",
      "<|startoftext|>- Камосвуребыжянию м кнетет ДуЗа бетиль сназыл сикпветичсиренынркапно в едтая В й тобыльшь.- Ба сал в тебото пнетем вознь комцелатьчток а праз Бчапу- Упрепрьдиго паич апрер \n",
      "----\n",
      "iter 9000, loss: 55.515869\n",
      "----\n",
      " дени упецин, чту мын.- Сошем вочта.\n",
      "\n",
      "<|startoftext|>Усриячини- А делеессцюенлать кра окасенясь.\n",
      "\n",
      "<|startoftext|>Зсе устьпомех ведите кур адешь снцинель.\n",
      "\n",
      "<|startoftext|>- завуз в босчий чемушешо весла \n",
      "----\n",
      "iter 10000, loss: 54.433625\n"
     ]
    }
   ],
   "source": [
    "while n < 10**4 + 1000: #True \n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    if p+seq_length+1 >= len(text) or n == 0: \n",
    "        hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "        p = 0 # go from start of data\n",
    "    inputs = [char_to_ix[ch] for ch in text[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in text[p+1:p+seq_length+1]]\n",
    "  \n",
    "    # sample from the model now and then\n",
    "    if n % 1000 == 0:\n",
    "        sample_ix = sample(hprev, inputs[0], 200)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print('----\\n %s \\n----' % (txt, ))\n",
    "  \n",
    "    # forward seq_length characters through the net and fetch gradient\n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    if n % 1000 == 0: \n",
    "        print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "    \n",
    "    # perform parameter update with Adagrad\n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                  [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                  [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "  \n",
    "    p += seq_length # move data pointer\n",
    "    n += 1 # iteration counter "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "ml_hw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
