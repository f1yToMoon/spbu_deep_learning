{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4eda323-3063-435f-bc1a-b1446b014c1c",
   "metadata": {},
   "source": [
    "# Простейшая рекуррентная сеть\n",
    "В этом ноутбуке мы пройдемся по основам работы с RNN. Сегодня займемся задачей генерации текста. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "70d8b089-5f9c-4dcb-8b14-3f565c24e438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from typing import Iterable, Tuple\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import clear_output\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198424b3-07c0-4b46-83f0-8bbb53acacd4",
   "metadata": {},
   "source": [
    "В качестве обучающего датасета возьмем набор из 120 тысяч анекдотов на русском языке. \n",
    "[Ссылка на данные](https://archive.org/download/120_tysyach_anekdotov) и [пост на хабре про тематическое моделирование](https://habr.com/ru/companies/otus/articles/723306/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "b5fda8b3-2e4b-4385-aad5-b10ad73a5d35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|startoftext|>Друзья мои, чтобы соответствовать вам, я готов сделать над собой усилие и стать лучше. Но тогда и вы станьте немного хуже!\\n\\n<|startoftext|>- Люся, ты все еще хранишь мой подарок?- Да.- Я думал, ты выкинула все, что со мной связано.- Плюшевый мишка не виноват, что ты ебл@н...\\n\\n<|startoftext|>- А вот скажи честно, ты во сне храпишь?- Понятие не имею, вроде, нет. От со'"
      ]
     },
     "execution_count": 518,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(r\"/home/an/Downloads/anek.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "text[118:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007f21a5-c7e3-445f-b902-24e18242bd7b",
   "metadata": {},
   "source": [
    "Мы не хотим моделировать все подряд, поэтому разобьем датасет на отдельные анекдоты.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "fddd3f65-a156-4bbd-8c56-078652d38ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_data(text):\n",
    "    return text.replace(\"\\n\\n\", \"\").split(\"<|startoftext|>\")[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "3ae42013-ef71-485c-805e-8cc4c61fe6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_text = cut_data(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "id": "67e8e214-e40c-4705-beb4-f51a6a284137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Друзья мои, чтобы соответствовать вам, я готов сделать над собой усилие и стать лучше. Но тогда и вы станьте немного хуже!',\n",
       " '- Люся, ты все еще хранишь мой подарок?- Да.- Я думал, ты выкинула все, что со мной связано.- Плюшевый мишка не виноват, что ты ебл@н...',\n",
       " '- А вот скажи честно, ты во сне храпишь?- Понятие не имею, вроде, нет. От собственного храпа по крайней мере еще ни разу не просыпался.- Ну, так у жены спроси.- А жена и подавно не знает. У нее странная привычка после замужества возникла: как спать ложится - беруши вставляет.',\n",
       " 'Поссорилась с мужем. Пока он спал, я мысленно развелась с ним, поделила имущество, переехала, поняла, что жить без него не могу, дала последний шанс, вернулась. В итоге, ложусь спать уже счастливой женщиной.',\n",
       " 'Если тебя посещают мысли о смерти - это еще полбеды. Беда - это когда смерть посещают мысли о тебе...']"
      ]
     },
     "execution_count": 521,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cut_text[1:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282f6226-74c6-4488-a7bc-9360437e1b1f",
   "metadata": {},
   "source": [
    "Сделаем для начала самую простую модель с токенами на уровне символов. Это значит, что каждому символу в тексте ставится в соответствие некоторое число. Некоторые способы токенизации используют части слов или, наоборот, части бинарного представления текста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "3e923efb-a3d5-4e22-b8e0-8bf6260d1e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('由', 'М', '给', ' ', 'у', '^', '已', 'С', '²', '»', 'ы', '副', '/', '\\ufeff', '老', 'ъ', 'N', '`', 'E', 'O', 'm', 'Е', 'P', '€', 'h', '“', 'U', 'ш', '虽', '&', 'и', '\\n', 'И', 'Б', '<', 'z', 'Я', 'ц', 'I', 'Ю', 'Г', '命', 'Р', 'Y', '̈', 'щ', '*', '经', 'X', '°', '”', 'в', 'G', 'π', 'а', '为', 'н', ':', 'ö', 'H', 'B', 'з', '−', 'ч', '应', ';', 'х', 'Ё', '́', '$', 'А', 't', 'Q', 'Ø', '3', '@', 'v', 'F', '0', 'i', 'Э', '7', '长', '¿', 'ο', 'э', 'К', 'u', '’', 'e', 'o', '的', '▒', '″', 'м', 'k', 'У', 'т', 'C', '=', '。', 'R', '2', 'V', 'Ъ', 'я', '.', '>', 'g', '名', 'L', 'M', 'Л', 'Н', '?', 'a', '成', 'ё', '̆', 'д', 'T', 'л', 'ф', 'Й', '举', '会', '选', 'З', 'J', 'Д', '#', 'П', 'Z', '结', '表', '6', '数', 'о', 's', 'б', '☺', '!', '-', 'j', '果', 'В', '9', 'Ч', 'ж', '×', '代', 'ю', '人', 'q', 'S', 'р', 'О', 'Ц', \"'\", 'е', 'Ж', '+', '事', 'Ь', '手', '理', '№', '然', 'Ф', 'd', 'ь', 'r', '新', 'p', '☻', 'ë', '接', '4', 'w', 'Х', ',', 'l', 'Ы', 'D', 'x', 'Ш', '8', '5', '_', '直', '|', 'K', '\\u200b', 'й', 'y', 'n', 'A', 'с', 'г', 'f', '\"', '并', 'b', '任', '，', 'п', 'c', 'Т', 'W', 'к', 'Щ', '1', '最', '%')\n"
     ]
    }
   ],
   "source": [
    "unique_chars = tuple(set(text))\n",
    "print(unique_chars)\n",
    "int2char = dict(enumerate(unique_chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99fa447-208d-4285-bb76-0870e985ce58",
   "metadata": {},
   "source": [
    "Напишем функции для энкодинга и декодинга нашего текста. Они будут преобразовывать список символов в список чисел и обратно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "97704441-98c6-4c16-b0c2-10b88f941c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[138, 169, 199, 138, 169, 199, 3, 115, 138, 169, 199, 115, 138, 169, 115, 138, 169, 95, 138, 181, 199, 3, 138, 169, 199, 95, 24, 3, 115, 138, 169, 199]\n",
      "sdfsdf asdfasdasdkslf sdfkh asdf\n"
     ]
    }
   ],
   "source": [
    "def encode(sentence, vocab):\n",
    "    l = []\n",
    "    for ch in sentence:\n",
    "        l.append(vocab[ch])\n",
    "    return l\n",
    "\n",
    "def decode(tokens, vocab):\n",
    "    l = \"\"\n",
    "    for tok in tokens:\n",
    "        l += vocab[tok]\n",
    "    return l\n",
    "\n",
    "a = encode(\"sdfsdf asdfasdasdkslf sdfkh asdf\", char2int)\n",
    "print(a)\n",
    "b = decode(a, int2char)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017baeba-1197-4d21-8cc8-28ccf43262c5",
   "metadata": {},
   "source": [
    "Просто представления символов в виде числа не подходят для обучения моделей. На выходе должны быть вероятности всех возможных токенов из словаря. Поэтому модели удобно учить с помощью энтропии. К тому же, токены часто преобразуют из исходного представления в эмбеддинги, которые также позволяют получить более удобное представление в высокоразмерном пространстве. \n",
    "\n",
    "В итоге векторы в модели выглядят следующим образом:\n",
    "![alt_text](../additional_materials/images/char_rnn.jfif)\n",
    "\n",
    "Задание: реализуйте метод, который преобразует батч в бинарное представление."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "e692112f-edea-4e18-b48b-5aec75f935d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(int_words: torch.Tensor, vocab_size: int) -> torch.Tensor:\n",
    "    \"\"\"Encodes batch of sentences into binary values\"\"\"\n",
    "    batch_size, seq_len = int_words.shape\n",
    "    words_one_hot = torch.zeros((batch_size, seq_len, vocab_size))\n",
    "    \n",
    "    for batch_idx in range(batch_size):\n",
    "        words_one_hot[batch_idx, torch.arange(seq_len), int_words[batch_idx]] = 1.0\n",
    "    return words_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88488683-6df3-430e-b942-9c10548a1802",
   "metadata": {},
   "source": [
    "Проверьте ваш код."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "af941c64-cc6d-41b4-92e3-a8f37b861545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "         [0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "         [0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 1., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "test_seq = torch.tensor([[2, 6, 4, 1], [0,3, 2, 4]])\n",
    "test_one_hot = one_hot_encode(test_seq, 8)\n",
    "\n",
    "print(test_one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da82134-e59d-4806-be2c-839c2f850ee6",
   "metadata": {},
   "source": [
    "Однако, наши последовательности на самом деле разной длины. Как же объединить их в батч?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c0fe1e-40a5-4a58-b1bd-b4a4101d986a",
   "metadata": {},
   "source": [
    "Реализуем два необходимых класса: \n",
    "- токенайзер, который будет брать текст, кодировать и декодировать символы. Еще одно, что будет реализовано там - добавлено несколько специальных символов (паддинг, конец последовательности, начало последовательности).\n",
    "- Датасет, который будет брать набор шуток, используя токенайзер, строить эмбеддинги и дополнять последовательность до максимальной длины."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "69d266c8-b4d0-42fd-9c6c-02b7f3b9a4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, text, max_len: int = 512):\n",
    "        self.text = text\n",
    "        self.max_len = max_len\n",
    "        self.specials = [\"<pad>\", \"<bos>\", \"<eos>\"]\n",
    "        self.unique_chars = list(set(text))\n",
    "        self._add_special(\"<pad>\")\n",
    "        self._add_special(\"<bos>\")\n",
    "        self._add_special(\"<eos>\")\n",
    "        \n",
    "        self.int2char = dict(enumerate(self.unique_chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "    \n",
    "    def _add_special(self, symbol) -> None:\n",
    "        if symbol not in self.unique_chars:\n",
    "            self.unique_chars.append(symbol)\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.unique_chars)\n",
    "        \n",
    "    def decode_symbol(self, el):\n",
    "        return self.int2char[el]\n",
    "        \n",
    "    def encode_symbol(self, el):\n",
    "        return self.char2int[el]\n",
    "        \n",
    "    def str_to_idx(self, chars):\n",
    "        return [self.encode_symbol(ch) for ch in chars]\n",
    "\n",
    "    def idx_to_str(self, idx):\n",
    "        return [self.decode_symbol(i) for i in idx]\n",
    "\n",
    "    def encode(self, chars):\n",
    "        chars = [\"<bos>\"] + list(chars) + [\"<eos>\"]\n",
    "        return self.str_to_idx(chars)\n",
    "\n",
    "    def decode(self, idx):\n",
    "        chars = self.idx_to_str(idx)\n",
    "        return ''.join([ch for ch in chars if ch not in [\"<bos>\", \"<eos>\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "b8a76399-0ae4-4d4f-9d95-56d695eb7f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class JokesDataset(Dataset):\n",
    "    def __init__(self, tokenizer, cut_text, max_len: int = 512):\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.cut_text = cut_text\n",
    "        self.pad_index = self.tokenizer.encode_symbol(\"<pad>\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cut_text)\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        joke = self.cut_text[item]\n",
    "        encoded_joke = self.tokenizer.encode(joke)\n",
    "\n",
    "        if len(encoded_joke) > self.max_len:\n",
    "            encoded_joke = encoded_joke[:self.max_len]\n",
    "\n",
    "        padding_num = self.max_len - len(encoded_joke)\n",
    "        padded_joke = encoded_joke + [self.pad_index] * padding_num\n",
    "\n",
    "        return torch.tensor(padded_joke), len(encoded_joke)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "af9e66a2-d196-459f-a88a-94bc119873e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(text)\n",
    "dataset = JokesDataset(tokenizer, cut_text, 512)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c72173d-d38b-4d4c-a98e-878267c0fd87",
   "metadata": {},
   "source": [
    "Вопрос: А как бы мы должны были разделять данные на последовательности и батчи в случае, если бы использовался сплошной текст?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bf1f16-53d0-45a6-abd5-1a4c5b17285f",
   "metadata": {},
   "source": [
    "Теперь реализуем нашу модель. \n",
    "Необходимо следующее:\n",
    " - Используя токенайзер, задать размер словаря\n",
    " - Задать слой RNN с помощью torch.RNN. Доп.задание: создайте модель, используя слой LSTM.\n",
    " - Задать полносвязный слой с набором параметров: размерность ввода — n_hidden; размерность выхода — размер словаря. Этот слой преобразует состояние модели в логиты токенов.\n",
    " - Определить шаг forward, который будет использоваться при обучении\n",
    " - Определить метод init_hidden, который будет задавать начальное внутреннее состояние. Инициализировать будем нулями.\n",
    " - Определить метод inference, в котором будет происходить генерация последовательности из префикса. Здесь мы уже не используем явные логиты, а семплируем токены на их основе.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "896c0f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer,\n",
    "        hidden_dim: int = 256,\n",
    "        num_layers: int = 2,\n",
    "        drop_prob: float = 0.5,\n",
    "        max_len: int = 512,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.n_hidden = hidden_dim\n",
    "        self.n_layers = num_layers\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        vocab_size = self.tokenizer.vocab_size\n",
    "        self.rnn = nn.RNN(input_size=vocab_size, hidden_size=self.n_hidden, num_layers=self.n_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(self.n_hidden, vocab_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, lengths: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size, seq_len = x.shape\n",
    "        h0 = torch.zeros(self.n_layers, batch_size, self.n_hidden).to(device)\n",
    "\n",
    "        one_hot = one_hot_encode(x, self.tokenizer.vocab_size)\n",
    "\n",
    "        packed_embeds = pack_padded_sequence(one_hot, lengths, batch_first=True, enforce_sorted=False).to(device)\n",
    "        packed_output, hidden = self.rnn(packed_embeds, h0)\n",
    "        out, lengths = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        out = self.dropout(out)\n",
    "        logits = self.fc(out)\n",
    "\n",
    "        return logits, hidden\n",
    "\n",
    "    def inference(self, prefix='<bos>'):\n",
    "        tokens = torch.tensor(self.tokenizer.encode(prefix)).unsqueeze(0).to(device)\n",
    "\n",
    "        while tokens.shape[1] < self.max_len:\n",
    "            lengths = torch.tensor([tokens.shape[1]], dtype=torch.int64)  \n",
    "            logits, hidden = self.forward(tokens, lengths)\n",
    "            logits = logits[:, -1, :]\n",
    "\n",
    "            new_token = torch.argmax(logits, dim=-1).unsqueeze(0)\n",
    "            tokens = torch.cat([tokens, new_token], dim=1)\n",
    "\n",
    "            if new_token.item() == self.tokenizer.encode_symbol('<eos>'):\n",
    "                break\n",
    "\n",
    "        return self.tokenizer.decode(tokens.squeeze().cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1202bfda-3653-4644-8fcc-eda9e92e434f",
   "metadata": {},
   "source": [
    "Зададим параметры для обучения. Можете варьировать их, чтобы вам хватило ресурсов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "173284d2-1d28-4235-a3ac-e25494039e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "seq_length = 512\n",
    "n_hidden = 64\n",
    "n_layers = 4\n",
    "drop_prob = 0.1\n",
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8329823d-abd8-4044-8206-470f07b6da62",
   "metadata": {},
   "source": [
    "Напишите функцию для одного тренировочного шага. В этом ноутбуке сам процесс обучения модели достаточно тривиален, поэтому мы не будем использовать сложные функции для обучающего цикла. Вы же, однако, можете дописать их."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb839274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(\n",
    "    model: CharRNN,\n",
    "    train_batch: Tuple[torch.Tensor, torch.Tensor],\n",
    "    vocab_size: int,\n",
    "    criterion: nn.Module,\n",
    "    optimizer,\n",
    "    device=\"cpu\"\n",
    ") -> torch.Tensor:\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    input_data, target_lengths = train_batch\n",
    "\n",
    "    # Перемещение данных на устройство\n",
    "    input_data = input_data.to(device)\n",
    "    target_lengths = target_lengths.to(device)\n",
    "\n",
    "    # Длина последовательностей\n",
    "    batch_size, seq_len = input_data.shape\n",
    "\n",
    "    # Получение выходов модели\n",
    "    logits, _ = model(input_data, target_lengths)\n",
    "\n",
    "    # Преобразование целевых данных и логитов\n",
    "    logits = logits.view(-1, vocab_size)\n",
    "    target_data = input_data.view(-1)\n",
    "\n",
    "    # Удаление padding с учетом длины последовательностей\n",
    "    mask = target_data != tokenizer.encode_symbol(\"<pad>\")\n",
    "    logits = logits[mask[:logits.size(0)]]  # Применяем маску только к размеру logits\n",
    "    target_data = target_data[mask[:logits.size(0)]]\n",
    "\n",
    "    # Проверки\n",
    "    print(f\"Logits after mask: {logits.shape}\")\n",
    "    print(f\"Target data after mask: {target_data.shape}\")\n",
    "\n",
    "    # Вычисление ошибки\n",
    "    loss = criterion(logits, target_data)\n",
    "\n",
    "    # Обратное распространение\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1055e6e-6374-4af3-b1ab-8ad8b4125664",
   "metadata": {},
   "source": [
    "Инициализируйте модель, функцию потерь и оптимизатор."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "f85fc024-7cec-4833-ac15-cafe05724003",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = CharRNN(tokenizer, n_hidden, n_layers, drop_prob).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a140de0c-e648-4d9f-babf-659486dbae92",
   "metadata": {},
   "source": [
    "Проверьте необученную модель: она должна выдавать бессмысленные последовательности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d26fdb-5292-41c4-83df-e8feb3a22561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: 然。然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然然\n"
     ]
    }
   ],
   "source": [
    "model.eval()  \n",
    "\n",
    "# prefix = \"rtfdg\"\n",
    "prefix = \"<bos>\"\n",
    "# prefix = \"<eos>\" \n",
    "generated_text = model.inference(prefix)\n",
    "print(\"Output:\", generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "37263cdf-5a6c-4612-8cf9-57105c169943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(losses):\n",
    "    clear_output()\n",
    "    plt.plot(range(1, len(losses) + 1), losses)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f70324-c88a-4d98-bae6-e6c5356f2025",
   "metadata": {},
   "source": [
    "Проведите обучение на протяжении нескольких эпох и выведите график лоссов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "57059744-44ac-4aad-86f6-f67dc2ea7600",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "The shape of the mask [16384] at index 0 does not match the shape of the indexed tensor [8992, 217] at index 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[535], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, train_batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[0;32m----> 9\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     12\u001b[0m     avg_epoch_loss \u001b[38;5;241m=\u001b[39m epoch_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader)\n",
      "Cell \u001b[0;32mIn[531], line 31\u001b[0m, in \u001b[0;36mtraining_step\u001b[0;34m(model, train_batch, vocab_size, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Удаление padding\u001b[39;00m\n\u001b[1;32m     30\u001b[0m mask \u001b[38;5;241m=\u001b[39m target_data \u001b[38;5;241m!=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode_symbol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<pad>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mlogits\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     32\u001b[0m target_data \u001b[38;5;241m=\u001b[39m target_data[mask]\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Проверки\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: The shape of the mask [16384] at index 0 does not match the shape of the indexed tensor [8992, 217] at index 0"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()  \n",
    "    epoch_loss = 0\n",
    "        \n",
    "    for batch_idx, train_batch in enumerate(dataloader):\n",
    "        loss = training_step(model, train_batch, tokenizer.vocab_size, criterion, optimizer, device)\n",
    "        epoch_loss += loss\n",
    "        \n",
    "        avg_epoch_loss = epoch_loss / len(dataloader)\n",
    "        losses.append(avg_epoch_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch}/{num_epochs}, Loss: {avg_epoch_loss:.4f}\")\n",
    "        \n",
    "        plot_losses(losses)\n",
    "        \n",
    "        torch.save(model.state_dict(), f\"rnn.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d8694a-132f-4a44-a5d3-a0f39219e55f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1人☻Kшш的%sе的%П人的%Ш%KП%ш%ш/шПKшш的Ш%Kшш1%KWK>K%的K人шK的%П的人%%ш人的%的人%шШшш事Ks人Ш人ш1的%П的%K%ш的的手%Kш人е1ш人ш%1的的%%%K的人ншШWШ%ш成人K人的Ш的%KеKWsшK人%ш的1☻手K人1的N%%人1%ш%шПш%З%%人П手KП人%%ш的шK的%З人%ш的的人>手人人人%ш事的%П人шNш%人%ш人ш人ш的Ш%KsПшш%ш的ш人%ШШ%шKшsш%人ш人%шШЗ的手еПше的%的П人%шK手的%的%шПKшш人KKкЗ人ш1Шш%s人%人ш人шш事%1K1Kш人%%ш☻%Пш人%%ш%人%шешK的%Пшш%K1%%ш%%☻K人的Пs手人шеШ6ПшK%人的%s%人1шш人%%шш的%Пш%人е人Шш人sПШsш人шП的人пKшs人人1ПKшKЗшП的%%的人人1Шe%шП人Р人/шK的j%K<的人人%ше人ПШj的人1的ш人K人1%%шПшK%ш的%%K的%☻KШПш人ш1的%шш的%%的人ш%шШ人к人☻П的的人的%人%NШ人的%的%ш的П%人е1人е%ш的人人ш%的Ш人%%П的%人кш人人ФШKKK%шKшП人',\n",
       " '1人%K人%%П手шK人%%ш%ш%Пш人шш%шш人KKШ的人1П的人%%Ш的%ПLППк的人人ш%的人人Пs%%手шПs1%ш的人Ш人Ш人人%ш人шПшs的%е人人人ППKшsш%K的%ш1%事KшП人ш%1KшW手的N人的%的%KшKЗ人шШ的☻ШП%ш%ПW人%人%шш手人%1的的%%Ш的%%ШШшППKшsш1пш%人KП%K%ш人的人%%☻Пш%П人кшП的N%П人K的%П人6шш的人sП☻%K人шшK%K人ш手1%шкш人ш%人1并☻ш%%ШППшK的шK人Ш人ШKK%%Шш并ш%的%ш的>人%N人人的ШK人П%шШшK1人%шеш☻ПsшK人1人的%的%Ш人ш人K%的%的KK手Ш人K%Пш人KШ手П人的人☻%ш的%☻人的%ШеПшKKK%ш的手%K人K%%ШшПш人人ш>ш手K1人的%K%ш的人%еШшK%手1ш手ПK1N人ш人的%的手人ш☻%KШш%人%шш%人ш的的WK人%人人手ППк%人ш的%人шш☻人шKKе%Шнш☻1人%ш的人的%Ш的%ш人的ПП的Шш的П人%K人N并Ш1Kш%的%人K的ш人的ш的N%%人sNШ☻П人N人ШKWП人Ш%шшшП☻%%ш的%шK人%K的ш%ш%шШ',\n",
       " 'е%人ш%%W1Nш%ш人的jK的%Kш人шK%шШ的☻ПKш的ш%KШ的人ш1的人人手%шш手W人%人шK人%K%KWш%K%K☻sKШ1人П%人%KKs%ш%%Ш人шП的%的K的K%%%的人ш%☻事ПшKш人人KШшШ人%ш%人的%ш的к%的☻%K的ш%ШKшш人KшшЁs人%ш人人K人ПNK%人ШKн的人%人>П人人1Шш人☻ППшK的%шs人/ПШшK%ш人的%K%☻的%人1的人%%Ш%шПшK的%З人人%ш%шKшШ手%П%N的☻%Пшш%的人%П人手%K的%K人ш的K手人%KШПKшKшKЗш的人%%Ш的ППKШ的%Kш%П人ш人的%的人人K6☻KK的人人的Kш%K/%%人Ш人ш%事的sK人%ш的的Ш%K人шK的%шШ%ш人%%s人шK%人Kш的人ШПKKK的人的人ш人的%Зs的%人Пш%ш的ш%%WП人шKшK的手%ш的ш%K%的☻gKK%ш☻шш的%П%Ш人%ш的的%%ш人的Ш的%%K的ППш%的%人☻KK%的%人☻%ш人11%ПK%K的手☻人☻%ш1人%Kш1☻еш手的%的%s%шш人шW的1Kes%%KшП人шKш的%Ш%%KKП人%Kш人人%П人ШП人的手шsKШ%ш人11的ы的Kш',\n",
       " 'KK%%人☻шШ人%ПK人%KШ%шK人K%ш人Ш%П人ШKш的Kш人☻%%%ПППш%ш人ш%人ШПн人%Nшg的%Пшшш结N的%1Ш1的ш人人1ш的%ш%1%шs人☻ПП人Ш1的K的%Ш1%ш人ш人кWШПKKKшш%ш%人☻%%Пш人K%K的人☻Пш%的%的%人%ш%K%%KшПш人的%人ш1的的手%KKKшKшш人ш1☻%Kш手ш人%Kшш人ш人%1的ПыK%П人KШ的%%K☻%KK人手人手П的е的Ш☻>ш%人шеN%Ш人шK的%K的%ш的人1П的%%ш的人шK%%ш1ш%%☻K手☻ПKш的的%K人шШ的1人KK☻Пш人ш1的人%KW手人Kш的人шП人шшN>人%%人%П%шшП☻Ksн人Kш1%ш%ш%人%KП人П%%ш人的%KKK%的%的%ШкшПШ的%%ш的%K的%шш的人N%Ш人KK的人%KВK人的ш的人1шШ%K手Kн人Шш%П人Kшs的KПП人1%Ш人ШП人%Пш的人的%пKKПшш%KK%人ш1☻KП的人K的Ш的%%ш人人%KшПKKШKшKшN人шK人KK1ш人%6的手的K%%ш人人并%ш的%шN☻%人%ПшKeкш人KW%ш人шK的%的人人的ШПW人K人ШШ的%KKx%шш人',\n",
       " '的Шs人%Пешш人s%ш的手1>K%ПшшNK的人☻П人П人K人的%☻%Шшш的W人Kш1人%Ш☻%K的%ш%ПешШ%шK人☻KШ的%人ш%%sе☻ш人人KK人人N人的W%%☻%eПП%шП%шШ人шПшs人%K/人шKN人%%☻的шПЗ%%人%П的ш的人的%%的人的%шП人Шш的人%6oПП人人的ПП手手人ш人N的%s的的人的%人的人ш人人ее%шшK人ш人̆人%%☻ПшK的%П的%шш人的%%人☻П%шшшш1%KKш人1KПш人%%K人шш%%%%П人Зш%З%ш的%ш%ш%%Ш的%K人%W1的%%ш☻шыш%Ш%%KП的%ПKшш人%ы人的K人%ш%的%%%ш人ш1Ёш的人%ШШшKKNш人人1%ш事的ш人%手/еш人人Ш%KKK手人%人☻K的%ш%шшKшN人ш%%☻人的%ш%的☻%ш的人%的人шKШШKшШ人N%☻П人П%%Пшшш%人ш1的人%K☻的%шK%ш%Пш人%K的ПK%☻的手人ш人人Шшs的KПш人☻人的%ПШш%人Ш人Ш人K人☻人ШшшK手%K人1的шЁ人шШк%шшЗW人%%K的шП人☻K☻KшПшKK1虽的人%Ф/шП手ШN人人%%的%手的K1Nш%人K1%шKП手的%K',\n",
       " '11的%KППK11☻K%K的%шN人☻е%K1шKK☻的%人人1Ш手ПППш人шKш人☻%%шKK%K的ш%ш的%人%%的%ПШ的%Kшш的N的%н%人кшш的%ш的П%н☻的NK%Nш☻KKKш人>ШШ的K人П人NK的%人Ш%шПШKK人ш人шNш人K人Kшs的%ПK的的к人K人人Ш手的KKK%шШ人K人手ш%ш的人Ш人ш人☻手的手的Kш%人☻K%手%KKш%%Ш人的кK的s人%шП人%П的%шш的%ешш的人Пш☻的ш%ш%的☻K%KеП的K手人%ш☻人NKе手Kшп的1%KкП的%>Kш的ш人人Ш人的П☻人шШ的%П人%П☻ППKш人Ш%Ш的%Ш的1WKKш人的%人ш的П人KШK的%1%/шП的ш%Kн%sШ人KK人手%шш%ШШш%%%KшK人K1%ш人%шшш的%KшП的%%K人人ш的%Nе☻的g人K%Ш%ш的人ш人N人K1s人KK的%K1шKшK的%人Ш%%ш%ш人ш人s%ш人%KШK人K1人KKK%Kш人Ш%人的人ПП人Ф的的%K%人%Kш%人%1人K%的е人j人的W1%并1人KП事ш人%%1的шKш%шK%П人ш的人%%ш人人KKK人%1шKкK1Kш人н1NшK%KKoПш人1%шш%',\n",
       " '1K%%K的KППWш人ш人ё%шNш%ш%手%ш人шПK%шшш1%R%%WППшs%K%ш%П人Kшш人1☻的ш的人ерш%Kеш人的手K%K人шш事%ш%П的%%ш人шKш%ш人шш的%%нШK的ш1人ШKK的KK的KтП人人K人%手人的K%sППШПп人人人ШПшK的☻%ш%s%☻ШшKKKшш的%%шШ%sш%шшш%П%ш的%̆ш手ЗKш%K%的ше人KшшШФ%K的人ПK☻%%%的ШшKш人Kы%ПшшK人%KшП人%1Ш的%П%шш人ШПs人ШK的%K的%шs的%%ш1ш%K☻П人1%的人人%ш的ышKш人шN人人%的ШшK人Ш的%ш人KKш的%☻人%ш人%6人ш%шKП%K的шш人1%K人%人ш手шП的NKKKш人Ш%%人的人шПшшшNП手人☻K%的%的%人ПKK手П人Ш的%的шш人☻KШ%人ШШПшKтsK人шш人шПк手人ш人的%%%ш人шsП%K人KKш%人☻ш%ш%Kы人K%%шш%%%шш人ш1的%K1的的KK结的人%ш%<Шк%人人Ш人П人N人的人Шш人шШ%%шПш%ш%的%Kш人人KK%ш人KШш%ш的人K%的人ШПшПеs%шKшs人%K%шшПш人人KП%шШ人%ш',\n",
       " '的%K的手人的%ПП%ШшK%手%ш的%Kеш人%1的%%Kш1人1%☻KK人ШKKЁ人%ПшП人jш的人шПшш的人1K人шKш%%%шшKK人K人KшN人%KШK事的K的%ш人的%s%KK%W人K%%的的%зKшП的%的人ш%П人шKш手K人的K%ш人%KW的☻1%KK人1Пш手%%的的gK%ш%1☻<人шШKKП人ш人шш的NШKK%%的%KKшшK1ш人ш人11人%Kш的的%ПKш人%K的%6%事☻шш人%Ш%人jШ☻K人K1的人☻Kш的%K的П人шKшsK%1☻KK%NK的人☻%ш的人ш人的ш人%K1ПKKKш的人手%%Пшеш的%人KKшs人ш1的%%%人g%KкШ人K%ш%ш手ш%KK%з☻%ш%шш人шK%%ШKк的人шKN人☻П人K的ш%%人ш1人к%%的手手ш的WK的%人%ш☻人☻%的人的ш人的jПK人%шшш%Kш的%人%ш的K人П人<П人K1ШшKшШ人%ш%Ш人шK1шш手%的ш1的K%П%N%шK%KшП人ш人的ш1ш人%s%ш的н☻K的шП<ШK的人K%шПшNK%K☻的人1人☻%K1人手шП的Ш人%人ш人☻шШ手%KШПш人кПпПшеШш%%jПшПшs的%%шШ人jKN',\n",
       " '%%1ш手%%%ШПK☻/ш的%的%%%☻K1шш的WKK1人的KеK人1%шШшK人K%的%шK%%к人%ШеПKK%人Kш的人ш%☻的%手е手%%ш手的KKKш☻人%K的%%Wш的%sП人ШK%的☻K%ш人шШ的的W人1人人%П的1П人%K人人NK的%%шШ人%人ш1е☻%%1的%П手%П人шПк手П的人☻KШП人Ш%Шш人的人ПKш%ШK%ш人ШKш人☻人ШшПL人ППшш人人>Ш的的ш的%ПKш人%Kш%人☻KШ%的Ш人%ш的%的人的%人к的%%Шшш%K%的%人%шgWПKш%人%sПш1的人%Ш☻的的%1人Kш的人%%ШППK%П人%П☻的%ПK的ш%人☻еш的%%шШ人☻KШшПK%шшш人的手KшK的%%人☻ПKшW人1%Ш%ш人人шK1%K%%ш人手W人%人%的Ш手шj人☻KШшШк%ш%%KПKшш人的н的K的%人шш的%1的шKшK人ш%Ш人的☻ШП人的%%ш的的K%N人%KШПKПsKшП人ш%шШK%KK11sшK1%1ПKшK%sшш%人%Kш%Ф%ш人шKПsшK的%%人的KП%☻的%ПK的%人%人%шШ人шKП人шшo的%ПK人шKш%N手1%K%ш%KWПш人K的的人ш人',\n",
       " '%%的K%的人的%ш1N人☻KK的的人1K人%шш的%1%шш%ш人的ш手%ШKшK人KШK%кK人П人%人K的人%%%ППK人ш人шш%s人%%人ПШ☻KПKKшKПKШШKш%人ШKKП的s的%1шк%K的的%%KшшшK1的人%Kеш的%%K人ш%шKK1%шПшШKш的人的%Ш%K人ш%s手ш%Пsш%ш%K人的手ш%jK的N%%%K人1шШWK%N人шПШ%ш%шШ%%K人KK%шше人o%%шш的е%ш人的1的%K%的%ш的的%事Ш人шK人oKшш事人шП的11%W%K☻的%%ш人的%%ш的的/>人W%П1的%%ШП人的KПK人Шш%шKшш%Ш☻ш的人KШ%人ШKПП%手шKшш人ШKK%%ш的%Kш人的%人s%П☻人шШП1ш人的%П>ш人KK的K1K的%人K1K☻Kш手的%%s的K%N%%П人%%%的NK的%☻人Ш%%П人шK的%%%шшП☻K的K人人шK的%%%шKш%ш人%шПш人%%的ПП%ш的人%人шKшK1s%Kш的%K%шш人☻1的人KKKП1☻1ешшsK人K%KШП%人人ш人人%K的%Ш人%KШWш人П1的%ш人的ШШ人ш人%П的ШшK人%Ш的Kш%K人的人шK的ш的%人']"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[model.inference(\"\") for _ in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef889dd2-fde2-429c-9c61-f5a93517bd3f",
   "metadata": {},
   "source": [
    "Теперь попробуем написать свой собственный RNN. Это будет довольно простая модель с одним слоем.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fe4954-e2b5-43c5-9bb6-0b2a5cc2cc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE: custom model nn.Module, changed CharRNN, etc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_hw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
